{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96b12de3",
   "metadata": {},
   "source": [
    "Define the AdaDelta Class base on the mathematic formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "74a5f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from EMA_class import EMA \n",
    "\n",
    "class AdaDelta:\n",
    "    def __init__(self, rho=0.95, constant=1e-6):\n",
    "        self.rho = rho\n",
    "        self.constant = constant\n",
    "        self.s = EMA(1 - self.rho, bias_correction=False) \n",
    "        self.delta_x = EMA(1 - self.rho, bias_correction=False) \n",
    "        \n",
    "    def update(self, params, gradients):\n",
    "\n",
    "        delta_x_value = self.delta_x.get_current_value()\n",
    "        if delta_x_value is None:\n",
    "            delta_x_value = np.zeros_like(params)\n",
    "        \n",
    "    \n",
    "        s_t = self.s.calculate(gradients**2)\n",
    "        rescaled_gradient = (np.sqrt(delta_x_value + self.constant) / \n",
    "                           np.sqrt(s_t + self.constant)) * gradients\n",
    "        \n",
    "        self.delta_x.calculate(rescaled_gradient**2)\n",
    "        new_params = params - rescaled_gradient\n",
    "        \n",
    "        \n",
    "        return new_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9bd080",
   "metadata": {},
   "source": [
    "Provide some function for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "13bb94b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaDelta Optimizer Demo\n",
      "\n",
      "Optimizing f(x,y) = (x-2)^2 + (y-3)^2\n",
      "=======================================================\n",
      "\n",
      "Step | x      | y      | Loss   | RMS[g] (x,y) | RMS[Δx] (x,y)\n",
      "------------------------------------------------------------\n",
      "   0 |  0.000 |  0.000 | 61.000 | (init,init)  | (init,init)\n",
      "   1 |  0.001 |  0.001 | 60.954 | (16.000,30.000) | (0.001,0.001)\n",
      "   2 |  0.002 |  0.002 | 60.889 | (15.996,29.995) | (0.002,0.002)\n",
      "   3 |  0.004 |  0.004 | 60.816 | (15.988,29.985) | (0.002,0.002)\n",
      "   4 |  0.006 |  0.006 | 60.737 | (15.978,29.973) | (0.002,0.002)\n",
      "   5 |  0.008 |  0.008 | 60.651 | (15.966,29.958) | (0.002,0.002)\n",
      "   6 |  0.010 |  0.010 | 60.560 | (15.953,29.941) | (0.002,0.002)\n",
      "   7 |  0.012 |  0.012 | 60.463 | (15.938,29.922) | (0.002,0.002)\n",
      "   8 |  0.014 |  0.014 | 60.360 | (15.922,29.903) | (0.002,0.002)\n",
      "   9 |  0.016 |  0.016 | 60.253 | (15.905,29.882) | (0.002,0.002)\n",
      "\n",
      "Final: x=0.018718, y=0.018729\n",
      "Final loss: 12.8134556209\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"AdaDelta Optimizer Demo\")\n",
    "    print(\"\\nOptimizing f(x,y) = (x-2)^2 + (y-3)^2\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Initial parameters\n",
    "    params = np.array([0.0, 0.0])\n",
    "    optimizer = AdaDelta(rho=0.5, constant=1e-6)\n",
    "    \n",
    "    print(\"\\nStep | x      | y      | Loss   | RMS[g] (x,y) | RMS[Δx] (x,y)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for step in range(10):\n",
    "        x, y = params[0], params[1]\n",
    "        \n",
    "        # Compute loss and gradients for f(x,y) = (x-2)^2 + (y-3)^2\n",
    "        loss = 4*(x - 2)**2 + 5*(y - 3)**2\n",
    "        gradients = np.array([8*(x - 2), 10*(y - 3)])\n",
    "\n",
    "        s_val = optimizer.s.get_current_value()\n",
    "        deltax_val = optimizer.delta_x.get_current_value()\n",
    "\n",
    "        if s_val is not None and deltax_val is not None:\n",
    "            rms_g = np.sqrt(s_val + optimizer.constant)\n",
    "            rms_dx = np.sqrt(deltax_val + optimizer.constant)\n",
    "            rms_g_str = f\"({rms_g[0]:.3f},{rms_g[1]:.3f})\"\n",
    "            rms_dx_str = f\"({rms_dx[0]:.3f},{rms_dx[1]:.3f})\"\n",
    "        else:\n",
    "            rms_g_str = \"(init,init)\"\n",
    "            rms_dx_str = \"(init,init)\"\n",
    "        \n",
    "        print(f\"{step:4d} | {x:6.3f} | {y:6.3f} | {loss:6.3f} | {rms_g_str:12} | {rms_dx_str}\")\n",
    "        \n",
    "        # Update parameters\n",
    "        params = optimizer.update(params, gradients)\n",
    "        \n",
    "        if loss < 1e-8:\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nFinal: x={params[0]:.6f}, y={params[1]:.6f}\")\n",
    "    print(f\"Final loss: {((params[0]-2)**2 + (params[1]-3)**2):.10f}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
