{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41452d47",
   "metadata": {},
   "source": [
    "Define Adam implementation base on mathematic formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f0a8106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from EMA_class import EMA\n",
    "\n",
    "class Adam:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # State variables\n",
    "        self.v = EMA(1- self.beta1, bias_correction=True)  # First moment estimate (momentum)\n",
    "        self.s = EMA(1 -self.beta2, bias_correction=True)  # Second moment estimate (variance)\n",
    "\n",
    "\n",
    "    def momentum_vector(self, params, gradients):\n",
    "        if isinstance(params, EMA):\n",
    "            return params.calculate(gradients)\n",
    "    \n",
    "\n",
    "        \n",
    "    def calculate(self, params, gradients):\n",
    "        # Initialize state on first call\n",
    "        v_hat = self.momentum_vector(self.v, gradients)\n",
    "        s_hat = self.momentum_vector(self.s, gradients**2)\n",
    "        rescaled_gradient = self.learning_rate * v_hat / (np.sqrt(s_hat) + self.epsilon)\n",
    "        new_params = params - rescaled_gradient\n",
    "        \n",
    "        return new_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3046d6dd",
   "metadata": {},
   "source": [
    "Additional improvement within Yogi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e0b202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam_Yogi:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # State variables\n",
    "        self.v = None # First moment estimate (momentum)\n",
    "        self.s = None  # Second moment estimate (variance)\n",
    "        self.t = 0     # Time step counter\n",
    "\n",
    "    \n",
    "\n",
    "    def calculate(self, params, gradients):\n",
    "        # Initialize state on first call\n",
    "        if self.v is None:\n",
    "            self.v = np.zeros_like(params)\n",
    "            self.s = np.zeros_like(params)\n",
    "        \n",
    "        # Increment time step\n",
    "        self.t += 1\n",
    "        self.v = self.v = self.beta1 * self.v + (1 - self.beta1) * gradients\n",
    "        self.s = self.s + (1 - self.beta2) * np.sign(gradients**2 - self.s) * gradients**2 # Yogi improvement\n",
    "        v_hat = self.v / (1 - self.beta1**self.t)\n",
    "        s_hat = self.s / (1 - self.beta2**self.t)\n",
    "        rescaled_gradient = self.learning_rate * v_hat / (np.sqrt(s_hat) + self.epsilon)\n",
    "        new_params = params - rescaled_gradient\n",
    "        \n",
    "        return new_params\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a814f29",
   "metadata": {},
   "source": [
    "Provide some function for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "befd1ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(optimize_algorithm, params):\n",
    "    for step in range(25):\n",
    "        x, y = params[0], params[1]\n",
    "        \n",
    "        # Compute loss and gradients for f(x,y) = (x-1)^2 + (y-2)^2\n",
    "        loss = (x - 1)**2 + (y - 2)**2\n",
    "        gradients = np.array([2*(x - 1), 2*(y - 2)])\n",
    "        \n",
    "        if optimize_algorithm.v is not None:\n",
    "            if isinstance(optimize_algorithm, Adam):\n",
    "                v_hat = optimize_algorithm.momentum_vector(optimize_algorithm.v, gradients)\n",
    "                s_hat = optimize_algorithm.momentum_vector(optimize_algorithm.s, gradients**2)\n",
    "            else:\n",
    "                v_hat = optimize_algorithm.v / (1 - optimize_algorithm.beta1**optimize_algorithm.t)\n",
    "                s_hat = optimize_algorithm.s / (1 - optimize_algorithm.beta2**optimize_algorithm.t)\n",
    "            v_hat_str = f\"({v_hat[0]:6.3f},{v_hat[1]:6.3f})\"\n",
    "            s_hat_str = f\"({s_hat[0]:6.3f},{s_hat[1]:6.3f})\"\n",
    "        else:\n",
    "            v_hat_str = \"(  init,  init)\"\n",
    "            s_hat_str = \"(  init,  init)\"\n",
    "        \n",
    "        if step % 5 == 0:\n",
    "            print(f\"{step:4d} | {x:6.3f} | {y:6.3f} | {loss:6.4f} | {v_hat_str} | {s_hat_str}\")\n",
    "        \n",
    "        params = optimize_algorithm.calculate(params, gradients)\n",
    "        \n",
    "        if loss < 1e-8:\n",
    "            break\n",
    "\n",
    "    print(f\"\\nFinal: x={params[0]:.6f}, y={params[1]:.6f}\")\n",
    "    print(f\"Final loss: {((params[0]-1)**2 + (params[1]-2)**2):.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5e83a669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam Optimizer Demo\n",
      "============================================================\n",
      "Optimizing f(x,y) = (x-1)^2 + (y-2)^2 with Adam\n",
      "\n",
      "Step | x      | y      | Loss   | first-moment estimate  | second-moment estimate\n",
      "-----------------------------------------------------------------\n",
      "   0 |  0.000 |  0.000 | 5.0000 | (-20.000,-40.000) | (4000.000,16000.000)\n",
      "   5 |  0.088 |  0.089 | 4.4834 | (-2.809,-5.723) | (365.118,1461.139)\n",
      "  10 |  0.161 |  0.164 | 4.0745 | (-2.026,-4.269) | (191.763,768.254)\n",
      "  15 |  0.236 |  0.244 | 3.6675 | (-1.731,-3.801) | (130.092,522.030)\n",
      "  20 |  0.312 |  0.330 | 3.2604 | (-1.538,-3.544) | (98.391,395.633)\n",
      "\n",
      "Final: x=0.390039, y=0.421850\n",
      "Final loss: 2.8626104041\n",
      "\n",
      "\n",
      "Adam Yogi Optimizer Demo\n",
      "============================================================\n",
      "Optimizing f(x,y) = (x-1)^2 + (y-2)^2 with Adam and Yogi improvement\n",
      "\n",
      "Step | x      | y      | Loss   | first-moment estimate  | second-moment estimate\n",
      "-----------------------------------------------------------------\n",
      "   0 |  0.000 |  0.000 | 5.0000 | (  init,  init) | (  init,  init)\n",
      "   5 |  0.492 |  0.497 | 2.5180 | (-1.561,-3.559) | ( 2.652,13.073)\n",
      "  10 |  0.923 |  0.974 | 1.0583 | (-0.968,-2.943) | ( 1.576,10.037)\n",
      "  15 |  1.201 |  1.409 | 0.3897 | (-0.405,-2.280) | ( 1.065, 7.693)\n",
      "  20 |  1.270 |  1.773 | 0.1245 | ( 0.015,-1.612) | ( 0.862, 5.990)\n",
      "\n",
      "Final: x=1.181505, y=2.041395\n",
      "Final loss: 0.0346574483\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    params = np.array([0.0, 0.0])\n",
    "    optimizer = Adam(learning_rate=0.1, beta1=0.9, beta2=0.999)\n",
    "    optimizer_yogi = Adam_Yogi(learning_rate=0.1, beta1=0.9, beta2=0.999)\n",
    "\n",
    "    print(\"Adam Optimizer Demo\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"Optimizing f(x,y) = (x-1)^2 + (y-2)^2 with Adam\")\n",
    "    print(\"\\nStep | x      | y      | Loss   | first-moment estimate  | second-moment estimate\")\n",
    "    print(\"-\" * 65)\n",
    "\n",
    "    calculate_gradient(optimizer, params)\n",
    "    print(\"\\n\")\n",
    "    print(\"Adam Yogi Optimizer Demo\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"Optimizing f(x,y) = (x-1)^2 + (y-2)^2 with Adam and Yogi improvement\")\n",
    "    print(\"\\nStep | x      | y      | Loss   | first-moment estimate  | second-moment estimate\")\n",
    "    print(\"-\" * 65)\n",
    "\n",
    "    calculate_gradient(optimizer_yogi, params)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba57c69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
