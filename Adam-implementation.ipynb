{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41452d47",
   "metadata": {},
   "source": [
    "Define Adam implementation base on mathematic formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f0a8106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Adam:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # State variables\n",
    "        self.v = None  # First moment estimate (momentum)\n",
    "        self.s = None  # Second moment estimate (variance)\n",
    "        self.t = 0     # Time step counter\n",
    "        \n",
    "    def update(self, params, gradients):\n",
    "        # Initialize state on first call\n",
    "        if self.v is None:\n",
    "            self.v = np.zeros_like(params)\n",
    "            self.s = np.zeros_like(params)\n",
    "        \n",
    "        # Increment time step\n",
    "        self.t += 1\n",
    "        \n",
    "\n",
    "        self.v = self.beta1 * self.v + (1 - self.beta1) * gradients\n",
    "        self.s = self.beta2 * self.s + (1 - self.beta2) * gradients**2\n",
    "        v_hat = self.v / (1 - self.beta1**self.t)\n",
    "        s_hat = self.s / (1 - self.beta2**self.t)\n",
    "        rescaled_gradient = self.learning_rate * v_hat / (np.sqrt(s_hat) + self.epsilon)\n",
    "        new_params = params - rescaled_gradient\n",
    "        \n",
    "        return new_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a814f29",
   "metadata": {},
   "source": [
    "Provide some function for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e83a669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam Optimizer Demo\n",
      "============================================================\n",
      "\n",
      "Example 1: Optimizing f(x,y) = (x-1)^2 + (y-2)^2\n",
      "Target: x=1, y=2\n",
      "\n",
      "Step | x      | y      | Loss   | v̂ (x,y)     | ŝ (x,y)\n",
      "-----------------------------------------------------------------\n",
      "   0 |  0.000 |  0.000 | 5.0000 | (  init,  init) | (  init,  init)\n",
      "   5 |  0.492 |  0.497 | 2.5169 | (-1.561,-3.559) | ( 2.645,13.043)\n",
      "  10 |  0.924 |  0.975 | 1.0556 | (-0.967,-2.942) | ( 1.565, 9.978)\n",
      "  15 |  1.202 |  1.412 | 0.3871 | (-0.403,-2.278) | ( 1.054, 7.614)\n",
      "  20 |  1.271 |  1.778 | 0.1230 | ( 0.018,-1.607) | ( 0.850, 5.899)\n",
      "\n",
      "Final: x=1.180554, y=2.046737\n",
      "Final loss: 0.0347842548\n",
      "\n",
      "Adam combines the benefits of:\n",
      "- Momentum (helps navigate ravines and accelerates convergence)\n",
      "- Adaptive learning rates (handles different parameter scales)\n",
      "- Bias correction (prevents initial steps from being too small)\n"
     ]
    }
   ],
   "source": [
    "# Example usage and demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Adam Optimizer Demo\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Example 1: Simple 2D quadratic optimization\n",
    "    print(\"\\nExample 1: Optimizing f(x,y) = (x-1)^2 + (y-2)^2\")\n",
    "    print(\"Target: x=1, y=2\")\n",
    "    \n",
    "    # Initial parameters\n",
    "    params = np.array([0.0, 0.0])\n",
    "    optimizer = Adam(learning_rate=0.1, beta1=0.9, beta2=0.999)\n",
    "    \n",
    "    print(\"\\nStep | x      | y      | Loss   | v̂ (x,y)     | ŝ (x,y)\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for step in range(25):\n",
    "        x, y = params[0], params[1]\n",
    "        \n",
    "        # Compute loss and gradients for f(x,y) = (x-1)^2 + (y-2)^2\n",
    "        loss = (x - 1)**2 + (y - 2)**2\n",
    "        gradients = np.array([2*(x - 1), 2*(y - 2)])\n",
    "        \n",
    "        # Show bias-corrected estimates for analysis\n",
    "        if optimizer.v is not None and optimizer.t > 0:\n",
    "            v_hat = optimizer.v / (1 - optimizer.beta1**optimizer.t)\n",
    "            s_hat = optimizer.s / (1 - optimizer.beta2**optimizer.t)\n",
    "            v_hat_str = f\"({v_hat[0]:6.3f},{v_hat[1]:6.3f})\"\n",
    "            s_hat_str = f\"({s_hat[0]:6.3f},{s_hat[1]:6.3f})\"\n",
    "        else:\n",
    "            v_hat_str = \"(  init,  init)\"\n",
    "            s_hat_str = \"(  init,  init)\"\n",
    "        \n",
    "        if step % 5 == 0:\n",
    "            print(f\"{step:4d} | {x:6.3f} | {y:6.3f} | {loss:6.4f} | {v_hat_str} | {s_hat_str}\")\n",
    "        \n",
    "        # Update parameters\n",
    "        params = optimizer.update(params, gradients)\n",
    "        \n",
    "        if loss < 1e-8:\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nFinal: x={params[0]:.6f}, y={params[1]:.6f}\")\n",
    "    print(f\"Final loss: {((params[0]-1)**2 + (params[1]-2)**2):.10f}\")\n",
    "    \n",
    "    \n",
    "    print(\"\\nAdam combines the benefits of:\")\n",
    "    print(\"- Momentum (helps navigate ravines and accelerates convergence)\")\n",
    "    print(\"- Adaptive learning rates (handles different parameter scales)\")\n",
    "    print(\"- Bias correction (prevents initial steps from being too small)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
