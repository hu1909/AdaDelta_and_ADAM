{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96b12de3",
   "metadata": {},
   "source": [
    "Define the AdaDelta Class base on the mathematic formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74a5f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class AdaDelta:\n",
    "    def __init__(self, rho=0.95, constant=1e-6):\n",
    "        self.rho = rho\n",
    "        self.constant = constant\n",
    "        self.s = None \n",
    "        self.delta_x = None  \n",
    "        \n",
    "    def update(self, params, gradients):\n",
    "        if self.s is None:\n",
    "            self.s = np.zeros_like(params)\n",
    "            self.delta_x = np.zeros_like(params)\n",
    "        \n",
    "    \n",
    "        self.s = self.rho * self.s + (1 - self.rho) * gradients**2\n",
    "        rescaled_gradient = (np.sqrt(self.delta_x + self.constant) / \n",
    "                           np.sqrt(self.s + self.constant)) * gradients\n",
    "        \n",
    "\n",
    "        new_params = params - rescaled_gradient\n",
    "        \n",
    "        \n",
    "        param_update = rescaled_gradient \n",
    "        self.delta_x = self.rho * self.delta_x + (1 - self.rho) * param_update**2\n",
    "        \n",
    "        return new_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9bd080",
   "metadata": {},
   "source": [
    "Provide some function for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bb94b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaDelta Optimizer Demo\n",
      "=======================================================\n",
      "\n",
      "Optimizing f(x,y) = (x-2)^2 + (y-3)^2\n",
      "\n",
      "Step | x      | y      | Loss   | RMS[g] (x,y) | RMS[Δx] (x,y)\n",
      "----------------------------------------------------------------------\n",
      "   0 |  0.000 |  0.000 | 13.000 | (init,init)  | (init,init)\n",
      "   1 |  0.004 |  0.004 | 12.955 | (0.894,1.342) | (0.001,0.001)\n",
      "   2 |  0.009 |  0.009 | 12.910 | (1.248,1.872) | (0.002,0.002)\n",
      "   3 |  0.014 |  0.014 | 12.865 | (1.507,2.262) | (0.002,0.002)\n",
      "   4 |  0.018 |  0.018 | 12.819 | (1.717,2.578) | (0.002,0.002)\n",
      "   5 |  0.023 |  0.023 | 12.774 | (1.893,2.845) | (0.002,0.002)\n",
      "   6 |  0.027 |  0.027 | 12.728 | (2.046,3.076) | (0.003,0.003)\n",
      "   7 |  0.032 |  0.032 | 12.682 | (2.181,3.279) | (0.003,0.003)\n",
      "   8 |  0.037 |  0.037 | 12.637 | (2.301,3.461) | (0.003,0.003)\n",
      "   9 |  0.041 |  0.041 | 12.591 | (2.408,3.624) | (0.003,0.003)\n",
      "  10 |  0.046 |  0.046 | 12.545 | (2.505,3.772) | (0.003,0.003)\n",
      "  11 |  0.050 |  0.051 | 12.500 | (2.594,3.907) | (0.003,0.003)\n",
      "  12 |  0.055 |  0.055 | 12.454 | (2.674,4.030) | (0.003,0.003)\n",
      "  13 |  0.060 |  0.060 | 12.409 | (2.748,4.143) | (0.003,0.003)\n",
      "  14 |  0.064 |  0.065 | 12.363 | (2.815,4.247) | (0.003,0.003)\n",
      "\n",
      "Final: x=0.068985, y=0.069270\n",
      "Final loss: 12.3179983805\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"AdaDelta Optimizer Demo\")\n",
    "    print(\"\\nOptimizing f(x,y) = (x-2)^2 + (y-3)^2\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Initial parameters\n",
    "    params = np.array([0.0, 0.0])\n",
    "    optimizer = AdaDelta(rho=0.95, constant=1e-6)\n",
    "    \n",
    "    print(\"\\nStep | x      | y      | Loss   | RMS[g] (x,y) | RMS[Δx] (x,y)\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for step in range(15):\n",
    "        x, y = params[0], params[1]\n",
    "        \n",
    "        # Compute loss and gradients for f(x,y) = (x-2)^2 + (y-3)^2\n",
    "        loss = (x - 2)**2 + (y - 3)**2\n",
    "        gradients = np.array([2*(x - 2), 2*(y - 3)])\n",
    "        \n",
    "        if optimizer.s is not None:\n",
    "            rms_g = np.sqrt(optimizer.s + optimizer.constant)\n",
    "            rms_dx = np.sqrt(optimizer.delta_x + optimizer.constant)\n",
    "            rms_g_str = f\"({rms_g[0]:.3f},{rms_g[1]:.3f})\"\n",
    "            rms_dx_str = f\"({rms_dx[0]:.3f},{rms_dx[1]:.3f})\"\n",
    "        else:\n",
    "            rms_g_str = \"(init,init)\"\n",
    "            rms_dx_str = \"(init,init)\"\n",
    "        \n",
    "        print(f\"{step:4d} | {x:6.3f} | {y:6.3f} | {loss:6.3f} | {rms_g_str:12} | {rms_dx_str}\")\n",
    "        \n",
    "        # Update parameters\n",
    "        params = optimizer.update(params, gradients)\n",
    "        \n",
    "        if loss < 1e-8:\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nFinal: x={params[0]:.6f}, y={params[1]:.6f}\")\n",
    "    print(f\"Final loss: {((params[0]-2)**2 + (params[1]-3)**2):.10f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4a1b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
